{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SINTA\\AppData\\Local\\Temp\\ipykernel_5028\\584406524.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n",
      "C:\\Users\\SINTA\\AppData\\Local\\Temp\\ipykernel_5028\\584406524.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firebase is already initialized\n",
      "Folder '20240619' created successfully.\n",
      "Firebase is already initialized\n",
      "Dataframe applications uploaded successfully as applications_20240619.csv!\n",
      "Dataframe articles uploaded successfully as articles_20240619.csv!\n",
      "Dataframe comments uploaded successfully as comments_20240619.csv!\n",
      "Dataframe donation_manual_comments uploaded successfully as donation_manual_comments_20240619.csv!\n",
      "Dataframe donation_manuals uploaded successfully as donation_manuals_20240619.csv!\n",
      "Dataframe fundraising_categories uploaded successfully as fundraising_categories_20240619.csv!\n",
      "Dataframe fundraisings uploaded successfully as fundraisings_20240619.csv!\n",
      "Dataframe likes_comments uploaded successfully as likes_comments_20240619.csv!\n",
      "Dataframe organizations uploaded successfully as organizations_20240619.csv!\n",
      "Dataframe testimoni_volunteers uploaded successfully as testimoni_volunteers_20240619.csv!\n",
      "Dataframe user_bookmark_fundraisings uploaded successfully as user_bookmark_fundraisings_20240619.csv!\n",
      "Dataframe user_bookmark_volunteer_vacancies uploaded successfully as user_bookmark_volunteer_vacancies_20240619.csv!\n",
      "Dataframe volunteers uploaded successfully as volunteers_20240619.csv!\n",
      "Dataframe users uploaded successfully as users_20240619.csv!\n",
      "Dataframe admins uploaded successfully as admins_20240619.csv!\n",
      "Dataframe like_donation_comments uploaded successfully as like_donation_comments_20240619.csv!\n",
      "Dataframe user_bookmark_articles uploaded successfully as user_bookmark_articles_20240619.csv!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, storage\n",
    "\n",
    "# Initialize raw_data list\n",
    "raw_data = []\n",
    "raw_data.clear()\n",
    "\n",
    "def ingestion(table):\n",
    "    load_dotenv()\n",
    "    connection = mysql.connector.connect(\n",
    "        host=os.getenv('HOST'),\n",
    "        user='root',\n",
    "        password=os.getenv('PASSWORD'),\n",
    "        database='capstone5'\n",
    "    )\n",
    "    query = f\"SELECT * FROM {table}\"  \n",
    "    df = pd.read_sql(query, connection)\n",
    "    connection.close()\n",
    "    return df\n",
    "\n",
    "# List of table names\n",
    "tables = [\n",
    "    'applications', 'articles', 'comments', 'donation_manual_comments',\n",
    "    'donation_manuals', 'fundraising_categories', 'fundraisings',\n",
    "    'likes_comments', 'organizations', 'testimoni_volunteers',\n",
    "    'user_bookmark_fundraisings', 'user_bookmark_volunteer_vacancies',\n",
    "    'volunteers', 'users', 'admins', 'like_donation_comments',\n",
    "    'user_bookmark_articles'\n",
    "]\n",
    "\n",
    "# Ingest data for all tables\n",
    "data_frame = [ingestion(table) for table in tables]\n",
    "\n",
    "def initialize_firebase():\n",
    "    load_dotenv()\n",
    "    if not firebase_admin._apps:\n",
    "        credentials_path = os.getenv('FIREBASE_CREDENTIALS_PATH')\n",
    "        cred = credentials.Certificate(credentials_path)\n",
    "        firebase_admin.initialize_app(cred)\n",
    "        print(\"Firebase has been initialized\")\n",
    "    else:\n",
    "        print(\"Firebase is already initialized\")\n",
    "    bucket_name = os.getenv('BUCKET_NAME')\n",
    "    return storage.bucket(bucket_name)\n",
    "\n",
    "def create_folder_in_bucket():\n",
    "    bucket = initialize_firebase()\n",
    "    current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    folder_blob = bucket.blob(f\"{current_date}/\")\n",
    "    folder_blob.upload_from_string('')\n",
    "    print(f\"Folder '{current_date}' created successfully.\")\n",
    "    return current_date\n",
    "\n",
    "def upload_dataframes_to_firebase(data_frames):\n",
    "    current_date = create_folder_in_bucket()\n",
    "    bucket = initialize_firebase()\n",
    "    \n",
    "    for df, table_name in zip(data_frames, tables):\n",
    "        # Convert dataframe to CSV string\n",
    "        csv_str = df.to_csv(index=False)\n",
    "\n",
    "        # Create the blob reference with folder name\n",
    "        file_name_with_date = f\"{table_name}_{current_date}.csv\"\n",
    "        file_path_in_bucket = f\"{current_date}/{file_name_with_date}\"\n",
    "        file_ref = bucket.blob(file_path_in_bucket)\n",
    "        \n",
    "        # Upload CSV string to Firebase\n",
    "        file_ref.upload_from_string(csv_str, content_type='text/csv')\n",
    "        print(f\"Dataframe {table_name} uploaded successfully as {file_name_with_date}!\")\n",
    "\n",
    "# Upload all dataframes to Firebase\n",
    "upload_dataframes_to_firebase(data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from io import StringIO\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set Google Cloud credentials\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "if credentials_path:\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n",
    "else:\n",
    "    raise Exception(\"GOOGLE_APPLICATION_CREDENTIALS is not set in the .env file\")\n",
    "\n",
    "# Create a BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Get dataset IDs from environment variables\n",
    "dataset_id_fact = os.getenv(\"dataset_id_fact\")\n",
    "dataset_id_dim = os.getenv(\"dataset_id_dim\")\n",
    "\n",
    "def load_df_to_bigquery(client, dataset_id, table_name, df):\n",
    "    # Convert DataFrame to CSV\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    table_id = f\"{dataset_id}.{table_name}\"\n",
    "    \n",
    "    # Configure partitioning if 'created_at' field exists in the DataFrame\n",
    "    partition_by = None\n",
    "    if 'created_at' in df.columns:\n",
    "        partition_by = bigquery.TimePartitioning(field=\"created_at\")\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1,\n",
    "        autodetect=True,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        time_partitioning=partition_by\n",
    "    )\n",
    "\n",
    "    # Load CSV data from StringIO buffer\n",
    "    job = client.load_table_from_file(csv_buffer, table_id, job_config=job_config)\n",
    "    \n",
    "    # Wait for the load job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Get table information\n",
    "    table = client.get_table(table_id)\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            table.num_rows, len(table.schema), table_id\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "def load_db_local(df):\n",
    "    # Establish the connection\n",
    "    connection = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='',\n",
    "        port=int(os.getenv('port'))\n",
    "    )\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Create database if it doesn't exist\n",
    "    cursor.execute('CREATE DATABASE IF NOT EXISTS peduli_pintar')\n",
    "    cursor.execute('USE peduli_pintar')\n",
    "\n",
    "    # Create the engine to connect to the database\n",
    "    engine = create_engine('mysql+pymysql://root:@localhost:3307/peduli_pintar')\n",
    "\n",
    "    # Convert the dataframe to SQL\n",
    "    df.to_sql(name='data_table', con=engine, if_exists='replace', index=False)\n",
    "    \n",
    "    # Add partitioning based on the created_at column\n",
    "    # Ensure created_at is in the correct datetime format in MySQL\n",
    "    cursor.execute('ALTER TABLE data_table MODIFY COLUMN created_at DATETIME')\n",
    "\n",
    "    # Create the partitioned table\n",
    "    cursor.execute('''\n",
    "        ALTER TABLE data_table\n",
    "        PARTITION BY RANGE (YEAR(created_at)) (\n",
    "            PARTITION p0 VALUES LESS THAN (2021),\n",
    "            PARTITION p1 VALUES LESS THAN (2022),\n",
    "            PARTITION p2 VALUES LESS THAN (2023),\n",
    "            PARTITION p3 VALUES LESS THAN (2024),\n",
    "            PARTITION p4 VALUES LESS THAN (2025),\n",
    "            PARTITION p5 VALUES LESS THAN (2026)\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    connection.commit()\n",
    "    connection.close()\n",
    "\n",
    "load_db_local()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
